{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dce0c8c-db45-4342-a427-06f9b4f278ed",
   "metadata": {},
   "source": [
    "**Start a new notebook in JupyterLab and import the libraries you’ll need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fbf381e-8e30-4fa6-a21d-43ccb37c9847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shahj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shahj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "# Download the stopwords data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bb637-13e6-43a2-a6b5-49f1899ead25",
   "metadata": {},
   "source": [
    "**Load the twentieth-century text file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4e0112-c999-44f8-b962-3d56c5679caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "file = open('key_events_20th_century.txt', 'r', encoding=\"utf-8\")\n",
    "\n",
    "# Read the contents\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc296a6c-a0ab-4a6b-8d53-e45a34ed979a",
   "metadata": {},
   "source": [
    "**Evaluating whether the text needs wrangling: are there any special characters used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05da8a4b-539a-49e4-afde-734a4c34fc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters used: {'県', ',', 'ã', '’', '?', ')', ';', '(', '縄', '.', ']', '-', '/', '–', 'ö', '^', 'í', '[', ':', 'é', '—', '|', '&', '沖', '／', '!', \"'\", '\"', '°', '®'}\n"
     ]
    }
   ],
   "source": [
    "def check_special_characters(text):\n",
    "    special_characters = set()\n",
    "    for char in text:\n",
    "        if char not in string.ascii_letters and char not in string.digits and char not in string.whitespace:\n",
    "            special_characters.add(char)\n",
    "    return special_characters\n",
    "\n",
    "#text = \"Your text goes here.\"\n",
    "special_chars = check_special_characters(text)\n",
    "print(\"Special characters used:\", special_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc9853-4289-415a-a679-465448738b99",
   "metadata": {},
   "source": [
    "**Inferences**\n",
    "\n",
    "{'縄', '\"', '!', ',', ']', '°', 'ö', 'ã', '県', '(', '[', '—', 'í', '|', '®', \"'\", '/', ';', 'é', '／', '&', '-', '^', '’', ')', ':', '–', '.', '沖', '?'}\n",
    "\n",
    "These are some of the special characters mentioned above that exist in the text. Therefore, there is a need for wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de628024-a405-4284-b2e9-4c7e6333ba10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performing wrangling\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bd1a52b-da5b-49ee-bb87-4d6cdecc7615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove numbers using regular expression\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b4e987b-975b-408b-81b6-5b268beced35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45bec2e-ef40-4f1d-8254-8f75f985ef4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Remove punctuation\n",
    "tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "# Step 4: Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Step 5: Remove special characters using regular expressions\n",
    "tokens = [re.sub(r'[^a-zA-Z0-9]', '', word) for word in tokens]\n",
    "\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ef319f-a0c9-41e8-8167-6dccf422f9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 7: Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens]\n",
    "\n",
    "#print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1392e6ce-0257-411f-8e74-3a143521f92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "file = open('countries_list.txt', 'r')\n",
    "\n",
    "# Read the contents\n",
    "countries = file.read()\n",
    "countries=countries.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2be129-4421-419f-82c2-ff4a2dfa8654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries mentioned in the text: ['Albania', 'Algeria', 'Angola', 'Australia', 'Austria', 'Bangladesh', 'Belarus', 'Belgium', 'Bulgaria', 'Cambodia', 'Canada', 'Cape Verde', 'Cuba', 'Denmark', 'Egypt', 'Estonia', 'Finland', 'France', 'Germany', 'Ghana', 'Greece', 'Guinea', 'Guinea', 'Bissau', 'Hungary', 'India', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Japan', 'Kenya', 'Laos', 'Latvia', 'Lebanon', 'Libya', 'Lithuania', 'Luxembourg', 'Mexico', 'Moldova', 'Mongolia', 'Morocco', 'Mozambique', 'Netherlands', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Papua New Guinea', 'Philippines', 'Poland', 'Romania', 'Russia', 'São Tomé and Príncipe', 'Serbia', 'Seychelles', 'Singapore', 'Slovakia', 'Solomon Islands', 'South Africa', 'Spain', 'Sudan', 'Sweden', 'Thailand', 'Ukraine', 'United Kingdom', 'United States', 'Vietnam']\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a country name exists in the text\n",
    "def check_country_mentions(text, country_list):\n",
    "    mentions = []\n",
    "    for country in country_list:\n",
    "        if country.lower() in text.lower():  # Convert both to lowercase for case-insensitive comparison\n",
    "            mentions.append(country)\n",
    "    return mentions\n",
    "\n",
    "# Call the function to get the list of mentioned countries\n",
    "mentioned_countries = check_country_mentions(text, countries)\n",
    "\n",
    "# Print the mentioned countries\n",
    "print(\"Countries mentioned in the text:\", mentioned_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0e5d-31b0-46ea-84ae-fdea172e2e8c",
   "metadata": {},
   "source": [
    "**Inferences**\n",
    "\n",
    "Yes, the names of the countries in my list are the same as mentioned in the text we have scraped from the 20th Century Wikipedia Page. We carefully compared the country names in both the countries_list and the scraped text and found no issues that would require correction. To ensure accurate comparison, we made sure that both the countries_list and the text were in lowercase to avoid any discrepancies due to case sensitivity. The data wrangling process allowed us to clean and prepare the data, ensuring that the country names were in a consistent format, making the comparison straightforward. As a result, we can confidently proceed with the analysis, knowing that the country names are aligned and suitable for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e24ef7-8b91-4e1f-abe0-925bdac5afeb",
   "metadata": {},
   "source": [
    "**Use the text file to create a NER object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5108b9b1-fb36-47bd-99b7-5efe81da5d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "with open('Common_Countries.txt', 'w') as file:\n",
    "    # Iterate over the list and write each element to a new line in the file\n",
    "    file.write(str(mentioned_countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b131756-5afb-45ed-9239-d12d621af6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tag the words with Part-of-Speech (POS) tags\n",
    "tagged_words = nltk.pos_tag(lemmatized_words)\n",
    "\n",
    "# Use NER to extract named entities\n",
    "ner_object = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "# Print the NER object\n",
    "#print(ner_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d628e9f-689e-401e-bc55-4d6672633627",
   "metadata": {},
   "source": [
    "**Split the sentence entities from the NER object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ffc6131-172a-4651-afda-d768b712aff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], ['France', 'Italy', 'Russia'], ['Germany', 'Austria', 'Hungary', 'Bulgaria'], ['Russia'], ['Germany', 'Russia'], ['Germany'], ['Germany'], [], [], [], [], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Germany', 'Italy'], ['Germany', 'Germany'], ['Germany', 'Germany'], ['Austria', 'Austria', 'Germany'], [], [], [], [], ['Spain'], [], [], [], [], ['France', 'Poland'], ['Poland'], ['France', 'Germany', 'Poland', 'Poland'], [], [], [], [], ['Poland', 'Germany'], ['Estonia', 'Latvia', 'Lithuania', 'Finland'], ['Germany', 'Poland', 'Belgium', 'Netherlands', 'Luxembourg'], ['Belgium'], ['Denmark', 'Norway'], ['Norway'], ['Norway', 'Denmark', 'Sweden', 'Germany'], ['France'], [], ['France'], [], [], ['France'], [], ['Italy'], [], ['Germany'], [], [], [], [], [], [], [], [], [], ['Greece', 'Albania', 'Greece'], [], [], [], ['Ukraine', 'Belarus'], [], [], ['Libya', 'Egypt'], ['Libya'], [], ['Egypt'], ['Iraq'], [], [], [], [], [], [], [], [], [], ['Japan', 'Germany'], ['Japan', 'Russia'], ['Germany', 'Italy'], [], ['Germany'], [], [], [], ['Morocco', 'Algeria'], ['Italy'], ['Italy', 'Italy'], [], [], ['Italy'], [], [], [], [], [], [], ['France'], ['Germany'], [], [], [], [], ['France'], [], [], [], [], [], [], [], [], [], [], [], ['France', 'Germany'], [], ['Poland', 'Germany'], ['Germany'], ['Germany'], [], [], [], ['Japan'], ['Japan'], [], ['Japan'], ['Japan'], ['Japan', 'Japan', 'Germany', 'Japan'], ['Japan'], ['Japan', 'Thailand', 'Singapore'], ['Japan', 'Solomon Islands'], ['Philippines', 'Japan'], [], [], ['Solomon Islands', 'Japan'], [], ['Japan'], ['Japan', 'India', 'Japan'], ['Japan'], ['Japan'], [], [], [], ['Japan'], ['Japan'], [], [], [], [], [], [], [], [], [], [], [], ['Poland'], [], [], ['Poland'], [], [], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], ['Japan'], [], [], [], [], ['Russia', 'France', 'India', 'Pakistan', 'Israel', 'South Africa', 'Israel'], ['Libya', 'Iran'], [], [], ['Germany', 'France'], [], ['Japan', 'Japan'], [], [], [], [], ['Papua New Guinea'], [], [], ['India'], ['India', 'India', 'Pakistan'], ['Philippines'], ['Laos', 'Cambodia', 'Kenya', 'Ghana'], ['Cape Verde', 'Angola', 'Mozambique'], [], [], [], [], [], [], ['Estonia', 'Lithuania', 'Finland', 'Romania'], ['Albania', 'Bulgaria', 'Poland', 'Romania', 'Hungary'], ['Mongolia', 'Cuba'], [], [], [], ['United States'], ['Canada', 'Australia'], [], ['Vietnam'], [], [], [], ['Cambodia'], [], ['Vietnam'], [], ['Cuba'], ['Cuba'], [], [], [], ['United States'], [], [], ['Russia', 'Japan'], [], [], [], [], [], ['Russia'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['India'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Austria'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Poland'], [], [], [], [], [], [], [], [], [], [], [], ['Russia', 'Finland'], [], [], ['Finland', 'Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Germany', 'Norway'], [], [], [], [], [], [], [], [], [], ['France'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Italy'], [], [], ['Greece'], [], [], [], [], [], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Germany'], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Italy'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Japan'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Italy', 'Japan'], [], [], [], [], [], [], ['Japan'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Philippines'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Japan'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Israel'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Israel'], [], [], [], [], [], [], [], [], [], ['Iran', 'Iran'], [], [], [], [], [], [], [], [], [], [], [], ['Germany'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['India'], [], [], [], ['India', 'Pakistan'], [], [], [], [], [], [], [], ['Philippines'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Romania', 'Russia'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Vietnam'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Vietnam'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['Lebanon'], [], [], [], [], ['India'], ['Singapore'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Open the file\n",
    "file = open('key_events_20th_century.txt', 'r', encoding=\"utf-8\")\n",
    "\n",
    "# Read the contents\n",
    "text = file.read()\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Function to check if the entity is in the countries list\n",
    "def is_country(entity):\n",
    "    return entity.text in countries  # Assuming countries_list is a list of countries\n",
    "\n",
    "# Split sentence entities\n",
    "sentence_entities = []\n",
    "for sentence in doc.sents:\n",
    "    sentence_entities.append([entity.text for entity in sentence.ents if is_country(entity)])\n",
    "\n",
    "print(sentence_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a5fce7-ddaa-4a13-90c9-fcf3a81e219b",
   "metadata": {},
   "source": [
    "**Filter the entities so that you end up only with the ones from your countries list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89ff7ff1-a232-4177-ba36-57fe4e73e975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Italy', 'Russia', 'Germany', 'Austria', 'Hungary', 'Bulgaria', 'Russia', 'Germany', 'Russia', 'Germany', 'Germany', 'Germany', 'Germany', 'Italy', 'Germany', 'Germany', 'Germany', 'Germany', 'Austria', 'Austria', 'Germany', 'Spain', 'France', 'Poland', 'Poland', 'France', 'Germany', 'Poland', 'Poland', 'Poland', 'Germany', 'Estonia', 'Latvia', 'Lithuania', 'Finland', 'Germany', 'Poland', 'Belgium', 'Netherlands', 'Luxembourg', 'Belgium', 'Denmark', 'Norway', 'Norway', 'Norway', 'Denmark', 'Sweden', 'Germany', 'France', 'France', 'France', 'Italy', 'Germany', 'Greece', 'Albania', 'Greece', 'Ukraine', 'Belarus', 'Libya', 'Egypt', 'Libya', 'Egypt', 'Iraq', 'Japan', 'Germany', 'Japan', 'Russia', 'Germany', 'Italy', 'Germany', 'Morocco', 'Algeria', 'Italy', 'Italy', 'Italy', 'Italy', 'France', 'Germany', 'France', 'France', 'Germany', 'Poland', 'Germany', 'Germany', 'Germany', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Germany', 'Japan', 'Japan', 'Japan', 'Thailand', 'Singapore', 'Japan', 'Solomon Islands', 'Philippines', 'Japan', 'Solomon Islands', 'Japan', 'Japan', 'Japan', 'India', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Poland', 'Poland', 'Germany', 'Japan', 'Russia', 'France', 'India', 'Pakistan', 'Israel', 'South Africa', 'Israel', 'Libya', 'Iran', 'Germany', 'France', 'Japan', 'Japan', 'Papua New Guinea', 'India', 'India', 'India', 'Pakistan', 'Philippines', 'Laos', 'Cambodia', 'Kenya', 'Ghana', 'Cape Verde', 'Angola', 'Mozambique', 'Estonia', 'Lithuania', 'Finland', 'Romania', 'Albania', 'Bulgaria', 'Poland', 'Romania', 'Hungary', 'Mongolia', 'Cuba', 'United States', 'Canada', 'Australia', 'Vietnam', 'Cambodia', 'Vietnam', 'Cuba', 'Cuba', 'United States', 'Russia', 'Japan', 'Russia', 'India', 'Austria', 'Poland', 'Russia', 'Finland', 'Finland', 'Germany', 'Germany', 'Norway', 'France', 'Italy', 'Greece', 'Germany', 'Germany', 'Germany', 'Germany', 'Italy', 'Japan', 'Italy', 'Japan', 'Japan', 'Philippines', 'Japan', 'Israel', 'Israel', 'Iran', 'Iran', 'Germany', 'India', 'India', 'Pakistan', 'Philippines', 'Romania', 'Russia', 'Vietnam', 'Vietnam', 'Lebanon', 'India', 'Singapore']\n"
     ]
    }
   ],
   "source": [
    "# Filter the entities to keep only those from the countries list\n",
    "filtered_entities = [entity.text for entity in doc.ents if is_country(entity)]\n",
    "\n",
    "print(filtered_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf479c3-b04b-4a01-aae6-5ee62fde7086",
   "metadata": {},
   "source": [
    "**Create the relationships dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdd8a3eb-a2ad-420b-ab3f-2e951ba70361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create relationships between countries\n",
    "relationships = []\n",
    "for i in range(len(filtered_entities) - 1):\n",
    "    for j in range(i + 1, len(filtered_entities)):\n",
    "        relationships.append((filtered_entities[i], filtered_entities[j]))\n",
    "\n",
    "# Step 3: Convert relationships list to DataFrame\n",
    "relationships_df = pd.DataFrame(relationships, columns=['Country1', 'Country2'])\n",
    "relationships_df.to_csv('Relationship.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc9726cc-26c2-4304-b022-811476bc671d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country1</th>\n",
       "      <th>Country2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France</td>\n",
       "      <td>Hungary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20701</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20702</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20703</th>\n",
       "      <td>Lebanon</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20704</th>\n",
       "      <td>Lebanon</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20705</th>\n",
       "      <td>India</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20706 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country1   Country2\n",
       "0       France      Italy\n",
       "1       France     Russia\n",
       "2       France    Germany\n",
       "3       France    Austria\n",
       "4       France    Hungary\n",
       "...        ...        ...\n",
       "20701  Vietnam      India\n",
       "20702  Vietnam  Singapore\n",
       "20703  Lebanon      India\n",
       "20704  Lebanon  Singapore\n",
       "20705    India  Singapore\n",
       "\n",
       "[20706 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20th_century",
   "language": "python",
   "name": "20th_century"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
